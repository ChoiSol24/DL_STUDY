# -*- coding: utf-8 -*-
"""DeepLearning_Week6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11aFbWEpCKmfINr-BXDH4I07nAhwNU0AJ
"""

import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader

training_data = datasets.FashionMNIST(
    root="data",
    train=True, 
    download=True,
    transform=ToTensor()
)

test_data = datasets.FashionMNIST(
  root = "data",
  train = False,
  download=True, 
  transform = ToTensor()
)

train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)

avg_loss_list = []

import torch
import torch.nn as nn
import torch.nn.functional as F

k = []
class MLP(torch.nn.Module):
  def __init__(self):
    super(MLP, self).__init__()
    self.linear_relu_stack = nn.Sequential(
        nn.Flatten(),
        nn.Linear(28*28, 512),
        nn.BatchNorm1d(512),
        nn.ReLU(),
        nn.Linear(512,512),
        nn.BatchNorm1d(512),
        nn.ReLU(),
        nn.Linear(512,10),
    )
  def forward(self, x) :
    logits = self.linear_relu_stack(x)
    return logits

device = 'cuda' if torch.cuda.is_available() else 'cpu'
lr = 0.01
epoches = 3
model = MLP().to(device)
criterion = torch.nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

for epoch in range(epoches):
  for batch_idx, (X_train, y_train) in enumerate(train_dataloader):
    model.train()
    optimizer.zero_grad()
    X_train = X_train.to(device)
    y_train = y_train.to(device)
    pred = model(X_train)
    loss = criterion(pred, y_train)
    loss.backward()
    optimizer.step()
    if batch_idx % 100 == 0:
      loss, current = loss.item(), batch_idx * len(X_train)
      print(f"loss: {loss:>7f} [{current:>5d}/{len(train_dataloader.dataset):>5d}")
  test_loss, correct = 0,0
  model.eval()
  with torch.no_grad():
    for X_test, y_test in test_dataloader : 
      X_test = X_test.to(device)
      y_test = y_test.to(device)
      pred = model(X_test)
      test_loss +=criterion(pred,y_test).item()
      correct += (pred.argmax(1) == y_test).type(torch.float).sum().item()
    test_loss /= len(test_dataloader)
    correct /= len(test_dataloader.dataset)
    print(f'Test Error: \n Accuracy:{(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\n')
    k.append(loss.data)
avg_loss_list.append(k)
print(optimizer)

avg_loss_list

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib
num = 1
for i in avg_loss_list :
  plt.plot(range(epoches), [e.to('cpu') for e in i], lw=1.5, label=str(num))
  num+=1
plt.legend(loc=0)
plt.savefig('test.png')

"class MLP(torch.nn.Module):
  def __init__(self):
    super(MLP, self).__init__()
    self.linear_relu_stack = nn.Sequential(
        nn.Flatten(),
        nn.Linear(28*28, 512),
        nn.ReLU(),
        nn.Linear(512,512),
        nn.ReLU(),
        nn.Linear(512,10),
    )
  def forward(self, x) :
    logits = self.linear_relu_stack(x)
    return logits

device = 'cuda' if torch.cuda.is_available() else 'cpu'
lr = 0.01
epoches = 20
model = MLP().to(device)
criterion = torch.nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=lr)

for epoch in range(epoches):
  for batch_idx, (X_train, y_train) in enumerate(train_dataloader):
    model.train()
    optimizer.zero_grad()
    X_train = X_train.to(device)
    y_train = y_train.to(device)
    pred = model(X_train)
    loss = criterion(pred, y_train)
    loss.backward()
    optimizer.step()
    if batch_idx % 100 == 0:
      loss, current = loss.item(), batch_idx * len(X_train)
      print(f"loss: {loss:>7f} [{current:>5d}/{len(train_dataloader.dataset):>5d}")
  test_loss, correct = 0,0
  model.eval()
  with torch.no_grad():
    for X_test, y_test in test_dataloader : 
      X_test = X_test.to(device)
      y_test = y_test.to(device)
      pred = model(X_test)
      test_loss +=criterion(pred,y_test).item()
      correct += (pred.argmax(1) == y_test).type(torch.float).sum().item()
    test_loss /= len(test_dataloader)
    correct /= len(test_dataloader.dataset)
    print(f'Test Error: \n Accuracy:{(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\n')

from torchvision import datasets, transforms
training_data = datasets.FashionMNIST(
    root="data",
    train=True, 
    download=True,
    transform=transforms.Compose(
        [transforms.ToTensor(),
         transforms.Normalize((0.5),(0.5))])

)

test_data = datasets.FashionMNIST(
  root = "data",
  train = False,
  download=True, 
  transform=transforms.Compose(
      [transforms.ToTensor(),
       transforms.Normalize((0.5),(0.5))])

)

train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)

class MLP(torch.nn.Module):
  def __init__(self):
    super(MLP, self).__init__()
    self.linear_relu_stack = nn.Sequential(
        nn.Flatten(),
        nn.Linear(28*28, 512),
        nn.BatchNorm1d(512),
        nn.ReLU(),
        nn.Linear(512,512),
        nn.BatchNorm1d(512),
        nn.ReLU(),
        nn.Linear(512,10),
    )
  def forward(self, x) :
    logits = self.linear_relu_stack(x)
    return logits

device = 'cuda' if torch.cuda.is_available() else 'cpu'
lr = 0.01
epoches = 20
model = MLP().to(device)
criterion = torch.nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=lr)

for epoch in range(epoches):
  for batch_idx, (X_train, y_train) in enumerate(train_dataloader):
    model.train()
    optimizer.zero_grad()
    X_train = X_train.to(device)
    y_train = y_train.to(device)
    pred = model(X_train)
    loss = criterion(pred, y_train)
    loss.backward()
    optimizer.step()
    if batch_idx % 100 == 0:
      loss, current = loss.item(), batch_idx * len(X_train)
      print(f"loss: {loss:>7f} [{current:>5d}/{len(train_dataloader.dataset):>5d}")
  test_loss, correct = 0,0
  model.eval()
  with torch.no_grad():
    for X_test, y_test in test_dataloader : 
      X_test = X_test.to(device)
      y_test = y_test.to(device)
      pred = model(X_test)
      test_loss +=criterion(pred,y_test).item()
      correct += (pred.argmax(1) == y_test).type(torch.float).sum().item()
    test_loss /= len(test_dataloader)
    correct /= len(test_dataloader.dataset)
    print(f'Test Error: \n Accuracy:{(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\n')