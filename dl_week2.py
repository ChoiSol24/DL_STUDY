# -*- coding: utf-8 -*-
"""DL_week2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WM6bJyUmIcza8ePmWmnZroPVOMoHHBSq
"""

from google.colab import auth
auth.authenticate_user()

from google.colab import drive
drive.mount('/content/gdrive')

!pip install torch
!pip install torchvision

# load an example dataset
from vega_datasets import data
cars = data.cars()

# plot the dataset, referencing dataframe column names
import altair as alt
alt.Chart(cars).mark_point().encode(
  x='Horsepower',
  y='Miles_per_Gallon',
  color='Origin'
).interactive()

import numpy as np
import pandas as pd
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784')
mnist.data.shape, mnist.target.shape
x_data = mnist.data
y_data = mnist.target

print(x_data.shape)
print(y_data.shape)

print(x_data.iloc[0])
print(y_data.iloc[0])

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
plt.imshow(x_data.iloc[0].values.reshape(28,28),cmap='gray')

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size=0.20, random_state=42)
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

model = LogisticRegression(max_iter=100, tol=0.0001, penalty='l2', C=1.0)

model.fit(X_train, Y_train)

Y_train_predict = model.predict(X_train)
train_acc = metrics.accuracy_score(Y_train, Y_train_predict)
print(train_acc)
Y_test_predict = model.predict(X_test)
test_acc = metrics.accuracy_score(Y_test, Y_test_predict)
print(test_acc)

#Deep Learning Homework (week 2)
import numpy as np
import pandas as pd
from sklearn import metrics 
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784')
mnist.data.shape, mnist.target.shape
x_data = mnist.data
y_data = mnist.target.astype(int)

import torch
import torch.nn as nn
import numpy as np
from sklearn.model_selection import train_test_split

device ='cpu'
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.20, random_state=42)
x_train, x_test = torch.Tensor(x_train.values), torch.Tensor(x_test.values)
y_train, y_test = torch.Tensor(y_train.values), torch.Tensor(y_test.values)
x_train = x_train.to(device)
x_test = x_test.to(device)
y_train = y_train.to(device)
y_test = y_test.to(device)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

class LogisticRegression(torch.nn.Module):
  def __init__ (self, input_dim, output_dim):
    super(LogisticRegression, self).__init__()
    self.linear = torch.nn.Linear(input_dim, output_dim)
  def forward(self, x):
    outputs = torch.sigmoid(self.linear(x))
    return outputs

epoches = 10000
input_dim = 784
output_dim = 10
lr = 0.01
model = LogisticRegression(input_dim, output_dim)
model = model.to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(),lr=lr)

loss_save_arr = []
for i in range(epoches):
  model.train()
  optimizer.zero_grad()
  output = model(x_train)
  loss = criterion(output, y_train.long())
  loss.backward()
  optimizer.step()
  loss_save_arr.append(loss.data)
  if(i%1000==0):
    print('epoch : ',i)
    print('loss : ', loss.data)
    _, pred = torch.max(output.data, axis=1)
    print("train_accuracy {:0.3f}".format(float((pred == y_train).sum())/y_train.size(0)))
    model.eval()
    with torch.no_grad():
      output = model(x_test)
      _, pred = torch.max(output.data, axis=1)
      print('test_accuracy {:0.3f}'.format(float((pred == y_test).sum())/y_test.size(0)))